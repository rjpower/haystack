{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "EMBED_MODEL = 'nomic-ai/nomic-embed-text-v1.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('mikex86/stackoverflow-posts', split='train', streaming=True)\n",
    "records = list(itertools.islice(iter(ds), 5))\n",
    "pd.DataFrame(records).iloc[0]['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(EMBED_MODEL)\n",
    "\n",
    "body = next(iter(ds))['Body']\n",
    "input_ids = tokenizer(body, return_tensors=\"np\")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModel.from_pretrained(EMBED_MODEL, trust_remote_code=True, safe_serialization=True)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model = model.to('mps')\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 128\n",
    "KEY_WINDOW = 32\n",
    "\n",
    "# TODO: Values should be added as tokens, not embeddings!\n",
    "VALUE_WINDOW = 64\n",
    "\n",
    "DS_KEYS = list(next(iter(ds)).keys())\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    mean = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "    mean = torch.nn.functional.layer_norm(mean, normalized_shape=(mean.shape[1],))\n",
    "    return mean[:, :EMBED_DIM]\n",
    "\n",
    "\n",
    "def split(record):\n",
    "    tokens = [tokenizer.tokenize(body) for body in record[\"Body\"]]\n",
    "    token_ids = tokenizer(record[\"Body\"], return_tensors=\"np\")\n",
    "\n",
    "    num_docs = token_ids[\"input_ids\"].shape[0]\n",
    "    result = []\n",
    "    for i in range(num_docs):\n",
    "        for j in range(0, token_ids[\"input_ids\"][i].shape[0], KEY_WINDOW):\n",
    "            result.append({\n",
    "                \"key_tokens\": np.array(tokens[i][j : j + KEY_WINDOW]),\n",
    "                \"key_input_ids\": token_ids[\"input_ids\"][i][j : j + KEY_WINDOW],\n",
    "                \"key_token_type_ids\": token_ids[\"token_type_ids\"][i][j : j + KEY_WINDOW],\n",
    "                \"key_attention_mask\": token_ids[\"attention_mask\"][i][j : j + KEY_WINDOW],\n",
    "                \"value_tokens\": np.array(tokens[i][j : j + VALUE_WINDOW]),\n",
    "                \"value_input_ids\": token_ids[\"input_ids\"][i][j : j + VALUE_WINDOW],\n",
    "                \"value_token_type_ids\": token_ids[\"token_type_ids\"][i][j : j + VALUE_WINDOW],\n",
    "                \"value_attention_mask\": token_ids[\"attention_mask\"][i][j : j + VALUE_WINDOW],\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    # iterate over columns,  convert to torch tensors, padding to the maximum length\n",
    "    result_dict = df.to_dict(orient=\"list\")\n",
    "\n",
    "    for k, v in result_dict.items():\n",
    "        if \"_tokens\" in k:\n",
    "            result_dict[k] = v\n",
    "            continue\n",
    "\n",
    "        max_len = max(len(array) for array in v)\n",
    "        # pad all tensors to max len and convert to 2d array\n",
    "        result_dict[k] = np.array([np.pad(array, (0, max_len - len(array))) for array in v])\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def encode(record):\n",
    "    splits = split(record)\n",
    "    for k, v in splits.items():\n",
    "        if \"_tokens\" in k:\n",
    "            continue\n",
    "        else:\n",
    "          splits[k] = torch.tensor(v).to(\"mps\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        key_embedding = model(\n",
    "            input_ids=splits[\"key_input_ids\"],\n",
    "            token_type_ids=splits[\"key_token_type_ids\"],\n",
    "            attention_mask=splits[\"key_attention_mask\"],\n",
    "        )\n",
    "        key_embedding = mean_pooling(key_embedding, splits[\"key_attention_mask\"])\n",
    "\n",
    "    key_embedding = key_embedding.detach().cpu().numpy()\n",
    "    print(len(splits[\"key_tokens\"]), key_embedding.shape, value_embedding.shape)\n",
    "\n",
    "    return {\n",
    "        \"key_embedding\": key_embedding,\n",
    "        \"value_input_ids\": splits[\"value_input_ids\"],\n",
    "        \"value_token_type_ids\": splits[\"value_token_type_ids\"],\n",
    "        \"value_attention_mask\": splits[\"value_attention_mask\"],\n",
    "        \"key_tokens\": splits[\"key_tokens\"],\n",
    "        \"value_tokens\": splits[\"value_tokens\"],\n",
    "    }\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "encode_ds = ds.map(\n",
    "    encode,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batched=True,\n",
    "    remove_columns=DS_KEYS,\n",
    ")\n",
    "\n",
    "encode_iter = iter(encode_ds)\n",
    "for i in range(1):\n",
    "    batch = next(encode_iter)\n",
    "    print('KEY_EMBED', batch['key_embedding'].mean())\n",
    "    print('KEY', ''.join(batch['key_tokens']))\n",
    "    print('VAL', ''.join(batch['value_tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the embedding performance.\n",
    "# Let's add 10000 documents to the ANN index and then query the index with a random document.\n",
    "\n",
    "import numpy as np\n",
    "import annoy\n",
    "\n",
    "index = annoy.AnnoyIndex(EMBED_DIM, \"angular\")\n",
    "\n",
    "records = []\n",
    "encode_iter = itertools.islice(iter(encode_ds), 10000)\n",
    "for i, batch in enumerate(tqdm.tqdm(encode_iter, total=10000)):\n",
    "    records.append(batch)\n",
    "    index.add_item(i, batch[\"key_embedding\"])\n",
    "\n",
    "index.build(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = df['key_embedding'].iloc[2]\n",
    "q.shape\n",
    "\n",
    "idx, dist = index.get_nns_by_vector(q, 5, search_k=100, include_distances=True)\n",
    "# make a dataframe of the closest docs\n",
    "df['key_tokens'].iloc[idx].str.join(' ').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
